{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“– TABLE OF CONTENTS\n",
        "\n",
        "- [1. LangChain: Building Context-Aware Reasoning Apps]()\n",
        "- [2. Installing the required libraries]()\n",
        "- [3. Mount Google Drive & Load API Keys]()\n",
        "- [4. The LLMs]()\n",
        "- [5. The Chains]()\n",
        "- [6. The Memory]()\n",
        "- [7. Deep Lake VectorStore]()\n",
        "  - [1. Create a Deep Lake dataset]()\n",
        "  - [2. Create an agent with a `RetrievalQA` chain as a tool to answer questions based on the given Deep Lake dataset]()\n",
        "  - [3. Reload existing vector store, Add more data, Recreate agent to answer questions based on the updated Deep Lake dataset]()\n",
        "- [8. Agents in LangChain]()\n",
        "- [9. Tools in LangChain]()"
      ],
      "metadata": {
        "id": "2Kph3Pq0KBgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. LangChain: Building Context-Aware Reasoning Apps"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). Here's a breakdown of its key features and functionalities:\n",
        "\n",
        "**Core Concept:**\n",
        "\n",
        "- **Leveraging LLMs:** LangChain allows you to integrate LLMs (like Gemini, ChatGPT etc.) into your applications to handle tasks like natural language processing, text generation, and code analysis.\n",
        "- **Focus on Context:** It goes beyond basic LLM integration by providing mechanisms to build context-aware applications. This means your app can understand the flow of information and user intent over time, leading to more meaningful interactions.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "- **Chains:** These are the building blocks of LangChain applications and represent sequences of steps that process information. Chains can involve various components like:\n",
        "    - **LLM calls:** Interact with the LLM to generate text, translate languages, write different kinds of creative content, or answer your questions in an informative way.\n",
        "    - **Data retrieval:** Access and process information from external sources like databases or APIs.\n",
        "    - **Code execution:** Perform calculations or manipulations within the application logic.\n",
        "- **Agents:** These represent actors within your application, such as the user or an external service. Chains can be triggered by specific actions from agents, creating a more dynamic and interactive experience.\n",
        "- **Retrieval Strategies:** LangChain allows you to define strategies for fetching information from external sources, allowing for flexibility in how your application retrieves relevant data.\n",
        "- **LangChain Expression Language:** This is a domain-specific language used to define the logic and flow of data within your LangChain application.\n",
        "\n",
        "**Benefits of Using LangChain:**\n",
        "\n",
        "- **Simplified LLM Integration:** LangChain streamlines the process of incorporating LLMs into your applications, reducing development complexity.\n",
        "- **Context-Aware Applications:** Build apps that understand the user's intent and the flow of information, leading to richer interactions.\n",
        "- **Vendor-Neutral Design:** LangChain works with various LLM providers, giving you flexibility in choosing the best model for your needs.\n",
        "- **Modular and Extensible:** The framework is designed for modularity, allowing you to easily add new functionalities and components as needed."
      ],
      "metadata": {
        "id": "0SxGqZoHZ7ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "Fv4x-kn2bvXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Installing the required libraries"
      ],
      "metadata": {
        "id": "iSMXICypS_jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain==0.0.208 deeplake openai==0.27.8 tiktoken python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c6qXOrjxSqa_",
        "outputId": "527a317e-cdd7-48ea-c5cc-04a0935b2738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deeplake\n",
            "  Downloading deeplake-3.9.12.tar.gz (606 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m606.9/606.9 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.10.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain==0.0.208)\n",
            "  Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.4)\n",
            "Collecting pillow~=10.2.0 (from deeplake)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.142-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Collecting libdeeplake==0.0.134 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.134-cp310-cp310-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-13.1.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Collecting dill (from libdeeplake==0.0.134->deeplake)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting aiobotocore[boto3]==2.13.1 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.13.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Collecting botocore<1.34.132,>=1.34.70 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.131-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.0.3)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.12-py3-none-any.whl size=729491 sha256=2fc1e8591420ebb10c74130111dfeec41cbf0ba817374cf931a027892457770e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/f4/28/0d59f27e5508ea19ce3dd781ea9ebbb502965cdf19939383d8\n",
            "Successfully built deeplake\n",
            "Installing collected packages: python-dotenv, pydantic, ppft, pox, pillow, mypy-extensions, marshmallow, lz4, jmespath, dill, aioitertools, aiofiles, typing-inspect, tiktoken, openapi-schema-pydantic, multiprocess, libdeeplake, langchainplus-sdk, humbug, botocore, s3transfer, pathos, openai, dataclasses-json, aiobotocore, langchain, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.0\n",
            "    Uninstalling pydantic-2.8.0:\n",
            "      Successfully uninstalled pydantic-2.8.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-13.1.1 aiobotocore-2.13.1 aiofiles-24.1.0 aioitertools-0.11.0 boto3-1.34.131 botocore-1.34.131 dataclasses-json-0.5.14 deeplake-3.9.12 dill-0.3.8 humbug-0.3.2 jmespath-1.0.1 langchain-0.0.208 langchainplus-sdk-0.0.20 libdeeplake-0.0.134 lz4-4.3.3 marshmallow-3.21.3 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pathos-0.3.2 pillow-10.2.0 pox-0.3.4 ppft-1.7.6.8 pydantic-1.10.17 python-dotenv-1.0.1 s3transfer-0.10.2 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "77588522dd514308b9c3501f96e62c97"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "v6uUrMvsbxfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Mount Google Drive & Load API Keys"
      ],
      "metadata": {
        "id": "offx4yVJZCZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY4yDKPpZGQs",
        "outputId": "a876b652-a128-4406-8fe3-40286d69ffa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the API Keys are stored in file \"llm_env\". It's contents are as below:\n",
        "\n",
        "ACTIVELOOP_TOKEN=<[Your Activeloop API Key](https://app.activeloop.ai/register)>\n",
        "\n",
        "OPENAI_API_KEY=<[Your OpenAI API Key](https://platform.openai.com/)>\n",
        "\n",
        "GOOGLE_API_KEY=<[Your Google API Key](https://console.cloud.google.com/apis/credentials)>\n",
        "\n",
        "GOOGLE_CSE_ID=<[Your Google Custom Search Engine ID](https://programmablesearchengine.google.com/controlpanel/create)>"
      ],
      "metadata": {
        "id": "YWHIdr_BeSwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Keys for Deep Lake Vector Database, OpenAI & Google\n",
        "load_dotenv('/content/drive/MyDrive/ancilcleetus-github/llm_env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWCwbr3UbsD4",
        "outputId": "a303f4dd-b8dd-4eae-ccf5-fdec295d1f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(f\"os.environ['ACTIVELOOP_TOKEN']: \\n{os.environ['ACTIVELOOP_TOKEN']}\")\n",
        "print(f\"os.environ['OPENAI_API_KEY']: \\n{os.environ['OPENAI_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_API_KEY']: \\n{os.environ['GOOGLE_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_CSE_ID']: \\n{os.environ['GOOGLE_CSE_ID']}\")"
      ],
      "metadata": {
        "id": "e2XY9JzccqGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "g_nRpWEBhVix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. The LLMs"
      ],
      "metadata": {
        "id": "GgObHqipcR6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fundamental component of LangChain involves invoking an LLM with a specific input. To illustrate this, we'll explore a simple example. Let's imagine we are building a service that suggests personalized workout routines based on an individual's fitness goals and preferences.\n",
        "\n",
        "To accomplish this, we will first need to import the LLM wrapper."
      ],
      "metadata": {
        "id": "Z7eFUUUhnlel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "eYWlS9OwnpcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature parameter in OpenAI models manages the randomness of the output. When set to 0, the output is mostly predetermined and suitable for tasks requiring stability and the most probable result. At a setting of 1.0, the output can be inconsistent and interesting but isn't generally advised for most tasks. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity. The best setting should be determined by experimenting with different values for each specific use case. The code initializes the GPT-3.5 modelâ€™s Turbo variant. We will learn more about the various models and their differences later on."
      ],
      "metadata": {
        "id": "O4fJXhV2sAnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
      ],
      "metadata": {
        "id": "SWqpQ0uksGta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNcgzalOsoKX",
        "outputId": "84d3c5fe-ee99-4b2d-ae5e-224998aec187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Monday:\n",
            "â€¢ Warm up: 5-minute jog or brisk walk\n",
            "â€¢ Main workout: 30 minutes of outdoor cycling\n",
            "â€¢ Cool down: 5-minute walk or light stretching\n",
            "\n",
            "Tuesday:\n",
            "â€¢ Warm up: 5-minute jog or brisk walk\n",
            "â€¢ Main workout: 45 minutes of hiking or trail running\n",
            "â€¢ Cool down: 5-minute walk or light stretching\n",
            "\n",
            "Wednesday:\n",
            "â€¢ Rest day or low-intensity activity such as yoga or swimming\n",
            "\n",
            "Thursday:\n",
            "â€¢ Warm up: 5-minute jog or brisk walk\n",
            "â€¢ Main workout: 20 minutes of interval training (alternating between 1 minute of sprinting and 1 minute of jogging)\n",
            "â€¢ Cool down: 5-minute walk or light stretching\n",
            "\n",
            "Friday:\n",
            "â€¢ Warm up: 5-minute jog or brisk walk\n",
            "â€¢ Main workout: 45 minutes of outdoor swimming or water aerobics\n",
            "â€¢ Cool down: 5-minute walk or light stretching\n",
            "\n",
            "Saturday:\n",
            "â€¢ Warm up: 5-minute jog or brisk walk\n",
            "â€¢ Main workout: 60 minutes of outdoor sports such as basketball, soccer, or tennis\n",
            "â€¢ Cool down: 5-minute walk or light stretching\n",
            "\n",
            "Sunday:\n",
            "â€¢ Rest day or low-intensity activity like leisurely walking or easy biking\n",
            "\n",
            "Tips:\n",
            "â€¢ Always start\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "kg2aWvVZhXcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. The Chains"
      ],
      "metadata": {
        "id": "hHaLRKZJhbuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the `LLMChain`, which consists of a `PromptTemplate`, a model (either an LLM or a ChatModel), and an optional output parser.\n",
        "\n",
        "The `LLMChain` works as follows:\n",
        "\n",
        "1. Takes (multiple) input variables.\n",
        "2. Uses the `PromptTemplate` to format the input variables into a prompt.\n",
        "3. Passes the formatted prompt to the model (LLM or ChatModel).\n",
        "4. If an output parser is provided, it uses the `OutputParser` to parse the output of the LLM into a final format."
      ],
      "metadata": {
        "id": "Vh50FFIghzqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's `LLMChain`, `PromptTemplate`, and `OpenAI` classes, we can easily define our prompt, set the input variables, and generate creative outputs."
      ],
      "metadata": {
        "id": "egzGrjyEiVSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain specifying only the input variable\n",
        "print(chain.run('eco-friendly water bottles'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2u2iajZifiV",
        "outputId": "9d127799-d3cc-4221-b7e0-18768cceaf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. \"Eco Hydrate\"\n",
            "2. \"GreenQuench\"\n",
            "3. \"Sustainable Sips\"\n",
            "4. \"EcoBottle Co.\"\n",
            "5. \"NatureNourish\"\n",
            "6. \"EcoH2O\"\n",
            "7. \"Pure Planet Bottles\"\n",
            "8. \"EcoFlask\"\n",
            "9. \"GreenWave Bottles\"\n",
            "10. \"EcoThirst\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example showcases the flexibility and ease of using LangChain to create custom chains for various language generation tasks."
      ],
      "metadata": {
        "id": "rmCzcYdnk_2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "wUG-r34ilD-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. The Memory"
      ],
      "metadata": {
        "id": "lpHqcyxnlogV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses. Memory, such as `ConversationBufferMemory`, acts as a wrapper around `ChatMessageHistory`, extracting the messages and providing them to the chain for better context-aware generation."
      ],
      "metadata": {
        "id": "WzD-rZdgsBjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with data analysis?\")\n",
        "\n",
        "# Display the conversation\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTgwejoKsIY-",
        "outputId": "e1443130-ee06-4c8d-9d2e-d25e045ef8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:  I have a wide range of capabilities, depending on the task at hand. I can perform simple tasks such as setting reminders and alarms, providing information on various topics, and even playing games. I can also assist with more complex tasks such as data analysis, language translation, and even driving autonomous vehicles. My abilities are constantly expanding as I learn and adapt to new situations and challenges. Is there something specific you would like me to demonstrate?\n",
            "Human: How can you help me with data analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.', additional_kwargs={}, example=False), AIMessage(content=' Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?', additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=' I have a wide range of capabilities, depending on the task at hand. I can perform simple tasks such as setting reminders and alarms, providing information on various topics, and even playing games. I can also assist with more complex tasks such as data analysis, language translation, and even driving autonomous vehicles. My abilities are constantly expanding as I learn and adapt to new situations and challenges. Is there something specific you would like me to demonstrate?', additional_kwargs={}, example=False), HumanMessage(content='How can you help me with data analysis?', additional_kwargs={}, example=False), AIMessage(content=' I can assist with data analysis by quickly sorting and organizing large amounts of data, identifying patterns and trends, and providing insights and recommendations based on the data. I can also perform complex calculations and simulations to help with decision making. Additionally, I can learn from the data and improve my analysis abilities over time. Would you like me to demonstrate my data analysis capabilities?', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-3.5-turbo-instruct', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this output, you can see the memory being used by observing the \"Current conversation\" section. After each input from the user, the conversation is updated with both the user's input and the AI's response. This way, the memory maintains a record of the entire conversation. When the AI generates its next response, it will use this conversation history as context, making its responses more coherent and relevant."
      ],
      "metadata": {
        "id": "1S_jaqHSuWU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "j5qG78N5uZiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Deep Lake VectorStore"
      ],
      "metadata": {
        "id": "r3DTQaQLwszF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain, facilitating the development and deployment of applications.\n",
        "\n",
        "Deep Lake provides several advantages over the typical vector store:\n",
        "- Itâ€™s **multimodal**, which means that it can be used to store items of diverse modalities, such as texts, images, audio, and video, along with their vector representations.\n",
        "- Itâ€™s **serverless**, which means that we can create and manage cloud datasets without creating and managing a database instance. This aspect gives a great speedup to new projects.\n",
        "- Last, itâ€™s possible to easily create a **data loader** out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow."
      ],
      "metadata": {
        "id": "FYIpnya2xDUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create a Deep Lake dataset"
      ],
      "metadata": {
        "id": "rPkwEYDiQd-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Instantiate the LLM and embeddings models\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Create our documents\n",
        "texts = [\n",
        "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
        "    \"Louis XIV was born in 5 September 1638\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# Create Deep Lake dataset\n",
        "my_activeloop_org_id = \"ancilcleetus\"  # By default, org_id is your Activeloop username\n",
        "my_activeloop_dataset_name = \"LangChain_101_Dataset\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# Add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kabdsb55ruFt",
        "outputId": "9f62652d-d3d6-419b-c0f2-14e9591723cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 2 embeddings in 1 batches of size 2:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://ancilcleetus/LangChain_101_Dataset', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (2, 1)      str     None   \n",
            " metadata     json      (2, 1)      str     None   \n",
            " embedding  embedding  (2, 1536)  float32   None   \n",
            "    id        text      (2, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0842111e-3ecf-11ef-989d-0242ac1c000c',\n",
              " '084213e4-3ecf-11ef-989d-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've just created your first Deep Lake dataset!"
      ],
      "metadata": {
        "id": "5WTHrG0I2WcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create an agent with a `RetrievalQA` chain as a tool to answer questions based on the given Deep Lake dataset"
      ],
      "metadata": {
        "id": "ogb9jqj7Qqw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a `RetrievalQA` chain:"
      ],
      "metadata": {
        "id": "lIlMgG3uQwnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever from the db\n",
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm,\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "HJBygmLpO2Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's create an agent that uses the `RetrievalQA` chain as a tool:"
      ],
      "metadata": {
        "id": "HDW-fEM6O_fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "\n",
        "# Instantiate a tool that uses the retriever\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Create an agent that uses the tool\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "TeG_f1SlPIxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can use the agent to ask a question:"
      ],
      "metadata": {
        "id": "_o4Fl9DXPhkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d59yk7SRPkJ8",
        "outputId": "af5ef867-12d5-4873-e310-1d8f22ca0b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to answer this question\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Napoleone born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\n",
            "Napoleon Bonaparte was born in 15 August 1769.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Napoleon Bonaparte was born in 15 August 1769.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Napoleon Bonaparte was born in 15 August 1769.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the agent used the â€œRetrieval QA Systemâ€ tool with the query â€œWhen was Napoleone born?â€ which is then run on our new Deep Lake dataset, returning the most similar document (i.e., the document containing the date of birth of Napoleon). This document is eventually used to generate the final output.\n",
        "\n",
        "This example demonstrates how to use Deep Lake as a vector database and create an agent with a `RetrievalQA` chain as a tool to answer questions based on the given document."
      ],
      "metadata": {
        "id": "pGK4KMwHP94W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Reload existing vector store, Add more data, Recreate agent to answer questions based on the updated Deep Lake dataset"
      ],
      "metadata": {
        "id": "iod3P_UzRE6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing Deep Lake dataset and specify the embedding function\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# Create new documents\n",
        "texts = [\n",
        "    \"Lady Gaga was born in 28 March 1986\",\n",
        "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# Add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwHvbDByePFK",
        "outputId": "ce0d0996-50ae-46d5-b450-ffd6ae3a0089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 2 embeddings in 1 batches of size 2:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://ancilcleetus/LangChain_101_Dataset', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (2, 1)      str     None   \n",
            " metadata     json      (2, 1)      str     None   \n",
            " embedding  embedding  (2, 1536)  float32   None   \n",
            "    id        text      (2, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9239d212-3ed9-11ef-989d-0242ac1c000c',\n",
              " '9239d4ec-3ed9-11ef-989d-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# Create a retriever from the db\n",
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm,\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=db.as_retriever()\n",
        ")\n",
        "\n",
        "# Instantiate a tool that uses the retriever\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Create an agent that uses the tool\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "1upv3MvvfAHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now test our agent with a new question."
      ],
      "metadata": {
        "id": "070v8yKRfwrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Michael Jordan born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siRDpfNXf2Le",
        "outputId": "805df1c2-6281-437e-f30b-44e2ab3254f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to find the answer.\n",
            "Action: Retrieval QA System\n",
            "Action Input: \"When was Michael Jordan born?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Michael Jordan was born on 17 February 1963.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM successfully retrieves accurate information by using the power of Deep Lake as a vector store and the OpenAI language model."
      ],
      "metadata": {
        "id": "O7_T71yGgKMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "BMIqtZ0O2Y-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Agents in LangChain"
      ],
      "metadata": {
        "id": "-Fd0inBFguJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
        "\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "Several types of agents are available in LangChain:\n",
        "\n",
        "- The **`zero-shot-react-description`** agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
        "\n",
        "- The **`react-docstore`** agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
        "\n",
        "- The **`self-ask-with-search`** agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
        "\n",
        "- The **`conversational-react-description`** agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions."
      ],
      "metadata": {
        "id": "9UJuuEDHhHH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our example, the Agent will use the Google Search tool to look up recent information about the Mars rover and generates a response based on this information.\n",
        "\n",
        "First, you want to set the environment variables â€œGOOGLE_API_KEYâ€ and â€œGOOGLE_CSE_IDâ€ to be able to use Google Search via API. Refer to [this article](https://python.langchain.com/v0.2/docs/integrations/tools/google_search/) for a guide on how to get them.\n",
        "\n",
        "**Note**\n",
        "\n",
        "The ID of your Google Custom Search Engine can be found in the code snippet you received after creating it. Look for the part where it says `cx=****` within the url in the `<script>` tag $\\implies$ `https://cse.google.com/cse.js?cx=GOOGLE_CSE_ID`"
      ],
      "metadata": {
        "id": "k8-EttSYhtCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API Keys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Keys for Deep Lake Vector Database, OpenAI & Google\n",
        "load_dotenv('/content/drive/MyDrive/ancilcleetus-github/llm_env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73607432-2456-4e63-cdec-1af79c7449ac",
        "id": "Cy5pzaOvnbq8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(f\"os.environ['ACTIVELOOP_TOKEN']: \\n{os.environ['ACTIVELOOP_TOKEN']}\")\n",
        "print(f\"os.environ['OPENAI_API_KEY']: \\n{os.environ['OPENAI_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_API_KEY']: \\n{os.environ['GOOGLE_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_CSE_ID']: \\n{os.environ['GOOGLE_CSE_ID']}\")"
      ],
      "metadata": {
        "id": "yUwpN7LanbrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's import the necessary modules:\n",
        "\n",
        "- **`langchain.llms.OpenAI`:** This is used to create an instance of the OpenAI language model, which can generate human-like text based on the input it's given.\n",
        "\n",
        "- **`langchain.agents.load_tools`:** This function is used to load a list of tools that an AI agent can use.\n",
        "\n",
        "- **`langchain.agents.initialize_agent`:** This function initializes an AI agent that can use a given set of tools and a language model to interact with users.\n",
        "\n",
        "- **`langchain.agents.Tool`:** This is a class used to define a tool that an AI agent can use. A tool is defined by its name, a function that performs the tool's action, and a description of the tool.\n",
        "\n",
        "- **`langchain.utilities.GoogleSearchAPIWrapper`:** This class is a wrapper for the Google Search API, allowing it to be used as a tool by an AI agent. It likely contains a method that sends a search query to Google and retrieves the results."
      ],
      "metadata": {
        "id": "pSTyu5xxo91E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, Tool, AgentType\n",
        "from langchain.utilities import GoogleSearchAPIWrapper"
      ],
      "metadata": {
        "id": "yi2Ycpvppfjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LLM and set the temperature to 0 for the precise answer\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)"
      ],
      "metadata": {
        "id": "oceg52xZp1L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Google Search API Wrapper\n",
        "search = GoogleSearchAPIWrapper()"
      ],
      "metadata": {
        "id": "biC8J3fRqGOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Tool` object represents a specific capability or function the system can use. In this case, it's a tool for performing Google searches.\n",
        "\n",
        "It is initialized with three parameters:\n",
        "\n",
        "- **`name`** parameter: This is a string that serves as a unique identifier for the tool. In this case, the name of the tool is \"google-search.â€\n",
        "\n",
        "- **`func`** parameter: This parameter is assigned the function that the tool will execute when called. In this case, it's the `run` method of the `search` object, which presumably performs a Google search.\n",
        "\n",
        "- **`description`** parameter: This is a string that briefly explains what the tool does. The description explains that this tool is helpful when you need to use Google to answer questions about current events."
      ],
      "metadata": {
        "id": "wwNq4AD9qaAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name = \"google-search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to search google to answer questions about current events\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "0LPjplGxr2aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create an agent that uses our Google Search tool:\n",
        "\n",
        "- `initialize_agent()`: This function call creates and initializes an agent. An agent is a component that determines which actions to take based on user input. These actions can be using a tool, returning a response to the user, or something else.\n",
        "\n",
        "- `tools`:  represents the list of `Tool` objects that the agent can use.\n",
        "\n",
        "- `agent=\"zero-shot-react-description\"`: The \"zero-shot-react-description\" type of an Agent uses the ReAct framework to decide which tool to use based only on the tool's description.\n",
        "\n",
        "- `verbose=True`: when set to True, it will cause the Agent to print more detailed information about what it's doing. This is useful for debugging and understanding what's happening under the hood.\n",
        "\n",
        "- `max_iterations=6`: sets a limit on the number of iterations the Agent can perform before stopping. It's a way of preventing the agent from running indefinitely in some cases, which may have unwanted monetary costs."
      ],
      "metadata": {
        "id": "vyd_yZIyr8PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=6)"
      ],
      "metadata": {
        "id": "MjNJrnuSszf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the response\n",
        "response = agent(\"What's the latest news about the Copa America?\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqEY7pJftGkH",
        "outputId": "38403d55-ab75-487d-a1db-279fde53ecc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use google-search to find the latest news about the Copa America.\n",
            "Action: google-search\n",
            "Action Input: \"Copa America news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mArgentina beat Canada 2-0 at the Copa AmÃ©rica on Tuesday night at MetLife Stadium in New Jersey, as goals from JuliÃ¡n Ãlvarez and Lionel Messi sent theÂ ... Breaking Copa America news and in-depth analysis from the best newsroom in sports. Follow your favorite clubs. Get the latest injury updates, player newsÂ ... The 2024 Copa America is underway in the United States. The defending champions Argentina and Lionel Messi are defending their title against the likes of BrazilÂ ... Explore all the essential information about the 2024 Copa America on the official Copa America website by Conmebol. Don't miss any important details. 6 days ago ... I guess it doesn't matter; it's the summer of soccer. Kyle Hayward. Latest News International Good Reads. Follow the passion for soccer on the Official Copa America Conmebol website with up-to-date news and information about the tournament. Emotion on the pitch! Copa America. News, information and last minute of the Copa America cup that will be held from June 20 to July 14, 2024 at Marca English. Mar 23, 2024 ... CONMEBOL Copa America 2024 groups are set, featuring exciting ... Related news. Imagem de capa. Colombia and Uruguay Battle For A SpotÂ ... Argentina: Your guide to historic Copa America semifinal showdown. By Kevin Nielsen Global News. Posted July 9, 2024 4:00 am. Updated July 9, 2024 10:07 pm. Jun 1, 2024 ... - CONMEBOL Copa AmÃ©rica 2024â„¢, the oldest national team tournament in the world, will begin in just 20 days and promises to be the most excitingÂ ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The latest news about the Copa America is that Argentina beat Canada 2-0 and is currently defending their title against Brazil in the 2024 tournament. The tournament will begin in 20 days and is expected to be very exciting.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The latest news about the Copa America is that Argentina beat Canada 2-0 and is currently defending their title against Brazil in the 2024 tournament. The tournament will begin in 20 days and is expected to be very exciting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, Agents in LangChain help decide which actions to take based on user input. The example demonstrates initializing and using a \"zero-shot-react-description\" agent with a Google search tool."
      ],
      "metadata": {
        "id": "2NUlZxU_vOSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "0xDnBNu12bOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Tools in LangChain"
      ],
      "metadata": {
        "id": "AV9Ay25mvUIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides a variety of tools for agents to interact with the outside world. These tools can be used to create custom agents that perform various tasks, such as searching the web, answering questions, or running Python code. In this section, we will discuss the different tool types available in LangChain and provide examples of creating and using them.\n",
        "\n",
        "In our example, two tools are being defined for use within a LangChain agent: a Google Search tool and a Language Model tool acting specifically as a text summarizer. The Google Search tool, using the GoogleSearchAPIWrapper, will handle queries that involve finding recent event information. The Language Model tool leverages the capabilities of a language model to summarize texts. These tools are designed to be used interchangeably by the agent, depending on the nature of the user's query."
      ],
      "metadata": {
        "id": "miAnW-JbwJ8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import initialize_agent, AgentType"
      ],
      "metadata": {
        "id": "O5FLRLmawaIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LLM and set the temperature to 0 for the precise answer\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Write a summary of the following text: {query}\"\n",
        ")\n",
        "\n",
        "# Instantiate LLMChain for Text Summarization\n",
        "summarize_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "bgW9nDzvwmSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tools for our agent\n",
        "search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for finding information about recent events\"\n",
        "    ),\n",
        "    Tool(\n",
        "       name='Summarizer',\n",
        "       func=summarize_chain.run,\n",
        "       description='useful for summarizing texts'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "4jD8KO3XxDpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our agent that leverages two tools\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "kwtDHCP_xbFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the agent with a question about summarizing the latest news about Copa America\n",
        "response = agent(\"What's the latest news about the Copa America 2024? Then please summarize the results.\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj_Rownhxq3J",
        "outputId": "fe30c93a-2067-49fa-a5cf-bf43e651d994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the search tool to find recent news about the Copa America 2024.\n",
            "Action: Search\n",
            "Action Input: \"Copa America 2024\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mFeatured Videos Â· Match Highlights: Uruguay (4) 0-0 (2) Brazil | CONMEBOL Copa AmÃ©rica USA 2024â„¢ Â· Match Highlights: Colombia 5-0 Panama | CONMEBOL Copa AmÃ©ricaÂ ... This is the second time that the United States is hosting the tournament, having hosted the Copa AmÃ©rica Centenario in 2016. Argentina is the defending championÂ ... Stay updated with the match schedule of 2024 Copa America of Conmebol on the official website. Don't miss any of the thrilling games. Explore now! 13 hours ago ... Messi's 109th goal leads defending champion Argentina over Canada 2-0 and into Copa America finalÂ ... Jun 10, 2024 ... Delta, LATAM Group to be the official airlines of Copa AmÃ©rica 2024 ... Delta and LATAM announced that they will be the official airlines of theÂ ... Jun 28, 2024 ... Lionel Messi can set a slew of records at the 2024 Copa America when he leads Argentina through the tournament. We'll track them all here. 20 hours ago ... Copa America 2024 - Semi-Final - Argentina vs. Canada Â· Copa America 2024 - Semifinal - Argentina vs. Canada Â· MetLife Stadium | Get DrivingÂ ... The 2024 Copa AmÃ©rica runs from June 20 to July 14. Check back here daily for results, news coverage and stories on the biggest names in the United States. May 1, 2024 ... Learn more about Copa America 2024 at Hard Rock Stadium on July 14, 2024. View the latest information about tickets and more. You can watch Copa America 2024 on Fubo (try for free). Here are the standings, schedule, and how to watch each game: Golazo Starting XIÂ ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now that I have found recent news about the Copa America 2024, I should use the summarizer tool to get a quick overview.\n",
            "Action: Summarizer\n",
            "Action Input: \"Featured Videos Â· Match Highlights: Uruguay (4) 0-0 (2) Brazil | CONMEBOL Copa AmÃ©rica USA 2024â„¢ Â· Match Highlights: Colombia 5-0 Panama | CONMEBOL Copa AmÃ©ricaÂ ... This is the second time that the United States is hosting the tournament, having hosted the Copa AmÃ©rica Centenario in 2016. Argentina is the defending championÂ ... Stay updated with the match schedule of 2024 Copa America of Conmebol on the official website. Don't miss any of the thrilling games. Explore now! 13 hours ago ... Messi's 109th goal leads defending champion Argentina over Canada 2-0 and into Copa America finalÂ ... Jun 10, 2024 ... Delta, LATAM Group to be the official airlines of Copa AmÃ©rica 2024 ... Delta and LATAM announced that they will be the official airlines of theÂ ... Jun 28, 2024 ... Lionel Messi can set a slew of records at the 2024 Copa America when he leads Argentina through the tournament. We'll track\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m his\n",
            "\n",
            "The text discusses the latest updates and highlights from the 2024 CONMEBOL Copa AmÃ©rica tournament. It mentions the match highlights of Uruguay vs Brazil and Colombia vs Panama, as well as the fact that this is the second time the United States is hosting the tournament. Argentina is the defending champion and their star player, Lionel Messi, scored his 109th goal to lead them to the final. The official website is the best source for staying updated on the match schedule. Delta and LATAM Group have been announced as the official airlines for the tournament. The text also mentions that Messi has the opportunity to set multiple records during the 2024 Copa America.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The latest news about the Copa America 2024 is that it is being hosted by the United States for the second time, with Argentina as the defending champion. Lionel Messi has the opportunity to set multiple records during the tournament. Delta and LATAM Group have been announced as the official airlines. The best source for staying updated on the match schedule is the official website.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The latest news about the Copa America 2024 is that it is being hosted by the United States for the second time, with Argentina as the defending champion. Lionel Messi has the opportunity to set multiple records during the tournament. Delta and LATAM Group have been announced as the official airlines. The best source for staying updated on the match schedule is the official website.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the agents used at first the â€œSearchâ€ tool to look for recent information about the Mars rover and then used the â€œSummarizerâ€ tool for writing a summary."
      ],
      "metadata": {
        "id": "DFWWjP_cyW9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides an expansive toolkit that integrates various functions to improve the functionality of conversational agents. Here are some examples:\n",
        "\n",
        "- **`SerpAPI`:** This tool is an interface for the SerpAPI search engine, allowing the agent to perform robust online searches to pull in relevant data for a conversation or task.\n",
        "\n",
        "- **`PythonREPLTool`:** This unique tool enables the writing and execution of Python code within an agent. This opens up a wide range of possibilities for advanced computations and interactions within the conversation.\n",
        "\n",
        "If you wish to add more specialized capabilities to your LangChain conversational agent, the platform offers the flexibility to create **`custom tools`**. By following the general tool creation guidelines provided in the LangChain documentation, you can develop tools tailored to the specific needs of your application."
      ],
      "metadata": {
        "id": "0LKDmWcVyaQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}
