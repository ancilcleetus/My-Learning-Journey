{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. LangChain: Building Context-Aware Reasoning Apps"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). Here's a breakdown of its key features and functionalities:\n",
        "\n",
        "Core Concept:\n",
        "\n",
        "- Leveraging LLMs: LangChain allows you to integrate LLMs (like Gemini, ChatGPT etc.) into your applications to handle tasks like natural language processing, text generation, and code analysis.\n",
        "- Focus on Context: It goes beyond basic LLM integration by providing mechanisms to build context-aware applications. This means your app can understand the flow of information and user intent over time, leading to more meaningful interactions.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "- Chains: These are the building blocks of LangChain applications and represent sequences of steps that process information. Chains can involve various components like:\n",
        "    - LLM calls: Interact with the LLM to generate text, translate languages, write different kinds of creative content, or answer your questions in an informative way.\n",
        "    - Data retrieval: Access and process information from external sources like databases or APIs.\n",
        "    - Code execution: Perform calculations or manipulations within the application logic.\n",
        "- Agents: These represent actors within your application, such as the user or an external service. Chains can be triggered by specific actions from agents, creating a more dynamic and interactive experience.\n",
        "- Retrieval Strategies: LangChain allows you to define strategies for fetching information from external sources, allowing for flexibility in how your application retrieves relevant data.\n",
        "- LangChain Expression Language: This is a domain-specific language used to define the logic and flow of data within your LangChain application.\n",
        "\n",
        "Benefits of Using LangChain:\n",
        "\n",
        "- Simplified LLM Integration: LangChain streamlines the process of incorporating LLMs into your applications, reducing development complexity.\n",
        "- Context-Aware Applications: Build apps that understand the user's intent and the flow of information, leading to richer interactions.\n",
        "- Vendor-Neutral Design: LangChain works with various LLM providers, giving you flexibility in choosing the best model for your needs.\n",
        "- Modular and Extensible: The framework is designed for modularity, allowing you to easily add new functionalities and components as needed."
      ],
      "metadata": {
        "id": "0SxGqZoHZ7ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "Fv4x-kn2bvXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Installing the required libraries"
      ],
      "metadata": {
        "id": "iSMXICypS_jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain==0.0.208 deeplake openai==0.27.8 tiktoken python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6qXOrjxSqa_",
        "outputId": "95023c22-5c80-4503-8c4f-7fc8d45d3a5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.0.208 in /usr/local/lib/python3.10/dist-packages (0.0.208)\n",
            "Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (3.9.9)\n",
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (0.5.14)\n",
            "Requirement already satisfied: langchainplus-sdk>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (0.0.20)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.10.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.25.2)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.4)\n",
            "Requirement already satisfied: pillow~=10.2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (10.2.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.34.106)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\n",
            "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.3.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Requirement already satisfied: libdeeplake==0.0.129 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.0.129)\n",
            "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (13.0.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.129->deeplake) (0.3.8)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: aiobotocore[boto3]==2.13.0 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake) (2.13.0)\n",
            "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake) (23.2.1)\n",
            "Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (1.34.106)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (0.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (0.10.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.0.3)\n",
            "Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (1.7.6.8)\n",
            "Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.3.4)\n",
            "Requirement already satisfied: multiprocess>=0.70.16 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.70.16)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (24.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.107,>=1.34.70->aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "v6uUrMvsbxfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Mount Google Drive"
      ],
      "metadata": {
        "id": "offx4yVJZCZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY4yDKPpZGQs",
        "outputId": "bd2eb203-3290-4fcf-a03e-0461143d7528"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "K0MI4UONbzTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Load API Keys"
      ],
      "metadata": {
        "id": "Nw3u9DkTXtRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activeloop and OpenAI API Keys are stored in file \".llm_env\". It's contents are as below:\n",
        "\n",
        "ACTIVELOOP_TOKEN=<[Your Activeloop API Key](https://app.activeloop.ai/register)>\n",
        "\n",
        "OPENAI_API_KEY=<[Your OpenAI API Key](https://platform.openai.com/)>"
      ],
      "metadata": {
        "id": "YWHIdr_BeSwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Keys for Data Lake Vector Database & OpenAI\n",
        "load_dotenv('/content/drive/MyDrive/ancilcleetus-github/.llm_env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWCwbr3UbsD4",
        "outputId": "c80fb7d0-c038-4cb9-85fa-18aa4b26dd16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(f\"os.environ['ACTIVELOOP_TOKEN']: \\n{os.environ['ACTIVELOOP_TOKEN']}\")\n",
        "print(f\"os.environ['OPENAI_API_KEY']: \\n{os.environ['OPENAI_API_KEY']}\")"
      ],
      "metadata": {
        "id": "e2XY9JzccqGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "g_nRpWEBhVix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. The LLMs"
      ],
      "metadata": {
        "id": "GgObHqipcR6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fundamental component of LangChain involves invoking an LLM with a specific input. To illustrate this, we'll explore a simple example. Let's imagine we are building a service that suggests personalized workout routines based on an individual's fitness goals and preferences.\n",
        "\n",
        "To accomplish this, we will first need to import the LLM wrapper."
      ],
      "metadata": {
        "id": "Z7eFUUUhnlel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "eYWlS9OwnpcY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature parameter in OpenAI models manages the randomness of the output. When set to 0, the output is mostly predetermined and suitable for tasks requiring stability and the most probable result. At a setting of 1.0, the output can be inconsistent and interesting but isn't generally advised for most tasks. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity. The best setting should be determined by experimenting with different values for each specific use case. The code initializes the GPT-3.5 modelâ€™s Turbo variant. We will learn more about the various models and their differences later on."
      ],
      "metadata": {
        "id": "O4fJXhV2sAnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
      ],
      "metadata": {
        "id": "SWqpQ0uksGta"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(f\"LLM Output: \\n{llm(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNcgzalOsoKX",
        "outputId": "4f8a134d-32f1-4847-920c-0b3e5b166323"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output: \n",
            "\n",
            "\n",
            "Here is a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities:\n",
            "\n",
            "1. Warm up: Start with a 5-10 minute brisk walk or jog to warm up your muscles and get your heart rate up.\n",
            "\n",
            "2. Interval Running: Find a flat road or trail and alternate between sprints and jogs for 20-30 minutes. Sprint for 30 seconds, then jog for 1-2 minutes. Repeat for the designated time.\n",
            "\n",
            "3. Hill Repeats: Find a steep hill and run up it at a consistent pace for 1-2 minutes. Jog back down and repeat for 3-5 sets.\n",
            "\n",
            "4. Stair Workout: Find a set of stairs or a stadium and run up and down them for 5-10 minutes. This will not only improve cardiovascular endurance but also add intensity to your workout.\n",
            "\n",
            "5. Hiking or Trail Running: Find a scenic trail and go for a hike or a trail run. This will not only improve your cardiovascular endurance but also challenge your balance and coordination.\n",
            "\n",
            "6. Cycling: Go for a bike ride in your neighborhood or on a designated bike path. You can also try increasing the intensity by adding hills or sprint intervals.\n",
            "\n",
            "7. Swimming: Head to a nearby pool or lake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "kg2aWvVZhXcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. The Chains"
      ],
      "metadata": {
        "id": "hHaLRKZJhbuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the `LLMChain`, which consists of a `PromptTemplate`, a model (either an LLM or a ChatModel), and an optional output parser.\n",
        "\n",
        "The `LLMChain` works as follows:\n",
        "\n",
        "1. Takes (multiple) input variables.\n",
        "2. Uses the `PromptTemplate` to format the input variables into a prompt.\n",
        "3. Passes the formatted prompt to the model (LLM or ChatModel).\n",
        "4. If an output parser is provided, it uses the `OutputParser` to parse the output of the LLM into a final format."
      ],
      "metadata": {
        "id": "Vh50FFIghzqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's `LLMChain`, `PromptTemplate`, and `OpenAI` classes, we can easily define our prompt, set the input variables, and generate creative outputs."
      ],
      "metadata": {
        "id": "egzGrjyEiVSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain specifying only the input variable\n",
        "print(f\"LLM Output: \\n{chain.run('eco-friendly water bottles')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2u2iajZifiV",
        "outputId": "8f6b28b1-4991-455b-9483-76b5007681ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output: \n",
            "\n",
            "\n",
            "1. \"EcoHydrate\"\n",
            "2. \"GreenWave Bottles\"\n",
            "3. \"Pure Planet Bottles\"\n",
            "4. \"EcoSip Co.\"\n",
            "5. \"Sustainable Swell\"\n",
            "6. \"EcoBottle Co.\"\n",
            "7. \"HydroEarth Co.\"\n",
            "8. \"EcoAqua\"\n",
            "9. \"GreenH2O Bottles\"\n",
            "10. \"EcoFlow Canteens\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example showcases the flexibility and ease of using LangChain to create custom chains for various language generation tasks."
      ],
      "metadata": {
        "id": "rmCzcYdnk_2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "wUG-r34ilD-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. The Memory"
      ],
      "metadata": {
        "id": "lpHqcyxnlogV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses. Memory, such as `ConversationBufferMemory`, acts as a wrapper around `ChatMessageHistory`, extracting the messages and providing them to the chain for better context-aware generation."
      ],
      "metadata": {
        "id": "WzD-rZdgsBjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with data analysis?\")\n",
        "\n",
        "# Display the conversation\n",
        "print(f\"Conversation: \\n{conversation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTgwejoKsIY-",
        "outputId": "5f975283-03d3-40ac-b0f9-732edb598be3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?\n",
            "Human: What can you do?\n",
            "AI:  I have a wide range of capabilities, depending on the task at hand. I can perform simple tasks such as setting reminders and alarms, providing information on various topics, and even playing games. I can also assist with more complex tasks such as data analysis, language translation, and even driving autonomous vehicles. My abilities are constantly expanding as I learn and adapt to new situations. Is there something specific you would like me to do for you?\n",
            "Human: How can you help me with data analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Conversation: \n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.', additional_kwargs={}, example=False), AIMessage(content=' Well, I am an artificial intelligence created by a team of programmers and engineers. My purpose is to assist and interact with humans in various tasks and conversations. I am constantly learning and improving through algorithms and data analysis. I am currently housed in a server room with other AI systems, constantly connected to the internet for information and updates. My programming is based on natural language processing, allowing me to understand and respond to human language in a conversational manner. Is there anything specific you would like to know about me?', additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=' I have a wide range of capabilities, depending on the task at hand. I can perform simple tasks such as setting reminders and alarms, providing information on various topics, and even playing games. I can also assist with more complex tasks such as data analysis, language translation, and even driving autonomous vehicles. My abilities are constantly expanding as I learn and adapt to new situations. Is there something specific you would like me to do for you?', additional_kwargs={}, example=False), HumanMessage(content='How can you help me with data analysis?', additional_kwargs={}, example=False), AIMessage(content=' I can help you with data analysis by quickly sorting through large amounts of data and identifying patterns and trends. I can also provide visual representations of the data to make it easier to understand. Additionally, I can make predictions and recommendations based on the data, helping you make informed decisions. Is there a specific dataset you would like me to analyze for you?', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-3.5-turbo-instruct', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='sk-proj-FKt0HRxeKGRyUBejnFbKT3BlbkFJGnJx3dB3acyzjyUFmTv3', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this output, you can see the memory being used by observing the \"Current conversation\" section. After each input from the user, the conversation is updated with both the user's input and the AI's response. This way, the memory maintains a record of the entire conversation. When the AI generates its next response, it will use this conversation history as context, making its responses more coherent and relevant."
      ],
      "metadata": {
        "id": "1S_jaqHSuWU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "j5qG78N5uZiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning as subset of ML\n",
        "\n",
        "from IPython import display\n",
        "display.Image(\"data/images/DL_01_Intro-01-DL-subset-of-ML.jpg\")"
      ],
      "metadata": {
        "id": "juNAxVBFg7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}