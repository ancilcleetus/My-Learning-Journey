{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📖 TABLE OF CONTENTS\n",
        "\n",
        "- [1. Introduction]()\n",
        "- [2. Installing Dependencies]()\n",
        "- [3. Mount Google Drive & Load API Keys]()\n",
        "- [4. Role Prompting]()\n",
        "  - [Example]()\n",
        "- [5. Few Shot Prompting]()\n",
        "- [6. Chain Prompting]()\n",
        "- [7. Bad Prompt Practices]()\n",
        "  - [1. Bad Prompt Example 1 - Open-ended Prompt]()\n",
        "  - [2. Bad Prompt Example 2 - Unclear Prompt]()\n",
        "- [8. Chain of Thought Prompting]()\n",
        "- [9. Tips for Effective Prompt Engineering]()"
      ],
      "metadata": {
        "id": "sxFOrkHyrQlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt engineering is a relatively new discipline that involves developing and optimizing prompts to use language models for various applications and research topics efficiently. It helps to understand the capabilities and limitations of LLMs better and is essential for many NLP tasks. We will provide practical examples to demonstrate the difference between good and bad prompts, helping you to understand the nuances of prompt engineering better.\n",
        "\n",
        "By the end of this lesson, you will have a solid foundation in the knowledge and strategies needed to create powerful prompts that enable LLMs to deliver accurate, contextually relevant, and insightful responses."
      ],
      "metadata": {
        "id": "ooZ1flRHy6kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "Fv4x-kn2bvXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Installing Dependencies"
      ],
      "metadata": {
        "id": "iSMXICypS_jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain==0.0.208 deeplake openai==0.27.8 python-dotenv tiktoken"
      ],
      "metadata": {
        "id": "c6qXOrjxSqa_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e0d9263-b7b6-4308-b1fa-d54316f2f23b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting deeplake\n",
            "  Downloading deeplake-3.9.15.tar.gz (607 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.10.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pydantic<2,>=1 (from langchain==0.0.208)\n",
            "  Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.4)\n",
            "Collecting pillow~=10.2.0 (from deeplake)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.148-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Collecting libdeeplake==0.0.137 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.137-cp310-cp310-manylinux2014_x86_64.whl.metadata (352 bytes)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-13.1.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Collecting dill (from libdeeplake==0.0.137->deeplake)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting aiobotocore==2.13.1 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.13.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting botocore<1.34.132,>=1.34.70 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.34.131-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.131-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.0.3)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pox>=0.3.4 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libdeeplake-0.0.137-cp310-cp310-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioboto3-13.1.1-py3-none-any.whl (34 kB)\n",
            "Downloading aiobotocore-2.13.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.34.131-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.15-py3-none-any.whl size=730462 sha256=0a5d3921a60d16360b2e47203a9ad5e08ea3b99744952f2d8c649f5c4d82db87\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/99/d8/463107fea3454d11c8c9cc8e621ff3a29a4790aeeb27496149\n",
            "Successfully built deeplake\n",
            "Installing collected packages: python-dotenv, pydantic, ppft, pox, pillow, mypy-extensions, marshmallow, lz4, jmespath, dill, aioitertools, aiofiles, typing-inspect, tiktoken, openapi-schema-pydantic, multiprocess, libdeeplake, langchainplus-sdk, humbug, botocore, s3transfer, pathos, openai, dataclasses-json, aiobotocore, langchain, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-13.1.1 aiobotocore-2.13.1 aiofiles-24.1.0 aioitertools-0.11.0 boto3-1.34.131 botocore-1.34.131 dataclasses-json-0.5.14 deeplake-3.9.15 dill-0.3.8 humbug-0.3.2 jmespath-1.0.1 langchain-0.0.208 langchainplus-sdk-0.0.20 libdeeplake-0.0.137 lz4-4.3.3 marshmallow-3.21.3 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pathos-0.3.2 pillow-10.2.0 pox-0.3.4 ppft-1.7.6.8 pydantic-1.10.17 python-dotenv-1.0.1 s3transfer-0.10.2 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "f65c378d07934be4a7113cb7a384b51a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "v6uUrMvsbxfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Mount Google Drive & Load API Keys"
      ],
      "metadata": {
        "id": "offx4yVJZCZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YY4yDKPpZGQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5102ce-11dc-4f4c-eb0a-16e540a96ca2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the API Keys are stored in file \"llm_env\". It's contents are as below:\n",
        "\n",
        "ACTIVELOOP_TOKEN=<[Your Activeloop API Key](https://app.activeloop.ai/register)>\n",
        "\n",
        "OPENAI_API_KEY=<[Your OpenAI API Key](https://platform.openai.com/)>\n",
        "\n",
        "GOOGLE_API_KEY=<[Your Google API Key](https://console.cloud.google.com/apis/credentials)>\n",
        "\n",
        "GOOGLE_CSE_ID=<[Your Google Custom Search Engine ID](https://programmablesearchengine.google.com/controlpanel/create)>\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN=<[Your Hugging Face Access Token](https://huggingface.co/settings/tokens)>"
      ],
      "metadata": {
        "id": "YWHIdr_BeSwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Keys for Deep Lake Vector Database, OpenAI, Google & Hugging Face\n",
        "load_dotenv('/content/drive/MyDrive/ancilcleetus-github/llm_env')"
      ],
      "metadata": {
        "id": "dWCwbr3UbsD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "621873f9-402b-4ce5-85d8-fe125362c6ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(f\"os.environ['ACTIVELOOP_TOKEN']: \\n{os.environ['ACTIVELOOP_TOKEN']}\")\n",
        "print(f\"os.environ['OPENAI_API_KEY']: \\n{os.environ['OPENAI_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_API_KEY']: \\n{os.environ['GOOGLE_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_CSE_ID']: \\n{os.environ['GOOGLE_CSE_ID']}\")\n",
        "print(f\"os.environ['HUGGINGFACEHUB_API_TOKEN']: \\n{os.environ['HUGGINGFACEHUB_API_TOKEN']}\")"
      ],
      "metadata": {
        "id": "e2XY9JzccqGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "o8Rfwu1JzSQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Role Prompting"
      ],
      "metadata": {
        "id": "96MuU8zizSQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Role prompting involves asking the LLM to assume a specific role or identity before performing a given task, such as acting as a copywriter. This can help guide the model's response by providing a context or perspective for the task. To work with role prompts, you could iteratively:\n",
        "\n",
        "1. Specify the role in your prompt, e.g., \"As a copywriter, create some attention-grabbing taglines for AWS services.\"\n",
        "2. Use the prompt to generate an output from an LLM.\n",
        "3. Analyze the generated response and, if necessary, refine the prompt for better results."
      ],
      "metadata": {
        "id": "dnlIPlltzgEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "OyYQsqH3zsyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the LLM is asked to act as a futuristic robot band conductor and suggest a song title related to the given theme and year."
      ],
      "metadata": {
        "id": "gevmJWRv0VWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "template = \"\"\"\n",
        "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
        "What's a cool song title for a song about {theme} in the year {year}?\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"theme\", \"year\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Input data for the prompt\n",
        "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
        "\n",
        "# Create LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the LLMChain to get the AI-generated song title\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(\"Theme: interstellar travel\")\n",
        "print(\"Year: 3030\")\n",
        "print(f\"AI-generated song title: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UHeJ2nK09Bz",
        "outputId": "ed13089e-6fad-4abc-b6a0-84da5e22453c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme: interstellar travel\n",
            "Year: 3030\n",
            "AI-generated song title: \n",
            "\"Galactic Odyssey: Journey to 3030\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a good prompt for several reasons:\n",
        "\n",
        "- **Clear instructions:** The prompt is phrased as a clear request for help in generating a song title, and it specifies the context: \"As a futuristic robot band conductor.\" This helps the LLM understand that the desired output should be a song title related to a futuristic scenario.\n",
        "\n",
        "- **Specificity:** The prompt asks for a song title that relates to a specific theme and a specific year, \"{theme} in the year {year}.\" This provides enough context for the LLM to generate a relevant and creative output. The prompt can be adapted for different themes and years by using input variables, making it versatile and reusable.\n",
        "\n",
        "- **Open-ended creativity:** The prompt allows for open-ended creativity, as it doesn't limit the LLM to a particular format or style for the song title. The LLM can generate a diverse range of song titles based on the given theme and year.\n",
        "\n",
        "- **Focus on the task:** The prompt is focused solely on generating a song title, making it easier for the LLM to provide a suitable output without getting sidetracked by unrelated topics.\n",
        "\n",
        "These elements help the LLM understand the user's intention and generate a suitable response."
      ],
      "metadata": {
        "id": "DRKowlC119mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "h9HBxlrK3Je3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Few Shot Prompting"
      ],
      "metadata": {
        "id": "jqiSb7UB3Je4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next example, the LLM is asked to provide the emotion associated with a given color based on a few examples of color-emotion pairs:"
      ],
      "metadata": {
        "id": "I-ikDKjf3YM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
        "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
        "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
        "]\n",
        "\n",
        "example_formatter_template = \"\"\"\n",
        "Color: {color}\n",
        "Emotion: {emotion}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"color\", \"emotion\"],\n",
        "    template=example_formatter_template,\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
        "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n",
        "\n",
        "formatted_prompt = few_shot_prompt.format(input=\"Yellow\")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt, input_variables=[]))\n",
        "\n",
        "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
        "response = chain.run({})\n",
        "\n",
        "print(\"Color: Yellow\")\n",
        "print(f\"Emotion: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttGTJLk03_vy",
        "outputId": "a38376fc-e1ac-4230-e2e8-f4bc8f64f7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color: Yellow\n",
            "Emotion:  happiness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt provides clear instructions and few-shot examples to help the model understand the task."
      ],
      "metadata": {
        "id": "HTm0z7nn4gAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "_g6Bx9564y4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Chain Prompting"
      ],
      "metadata": {
        "id": "HGLQFfIh4y4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain Prompting refers to the practice of chaining consecutive prompts, where the output of a previous prompt becomes the input of the successive prompt.\n",
        "\n",
        "To use chain prompting with LangChain, you could:\n",
        "\n",
        "- Extract relevant information from the generated response.\n",
        "- Use the extracted information to create a new prompt that builds upon the previous response.\n",
        "- Repeat steps as needed until the desired output is achieved.\n",
        "\n",
        "`PromptTemplate` class makes constructing prompts with dynamic inputs easier. This is useful when creating a prompt chain that depends on previous answers."
      ],
      "metadata": {
        "id": "W-YSLVfhkIjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(template=template_fact, input_variables=[\"scientist\"])\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(f\"Scientist: {scientist}\")\n",
        "print(f\"Fact: {response_fact}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4RvzPWOkSFm",
        "outputId": "1c4b356d-870f-4c66-ed8c-2921f569f631"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scientist: Albert Einstein\n",
            "Fact: \n",
            "Albert Einstein's theory of general relativity is a theory of gravity that explains how massive objects interact with each other and how they affect the fabric of space and time. According to this theory, gravity is not a force between masses, but rather a curvature of space and time caused by the presence of massive objects. This curvature is what causes objects to move towards each other, as if they were being pulled by a force. General relativity also predicts that the path of an object in space is affected by the presence of other massive objects, and that the passage of time is affected by the strength of gravity. This theory has been confirmed by numerous experiments and is considered one of the most important and influential theories in modern physics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "XgETPQc7lcNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Bad Prompt Practices"
      ],
      "metadata": {
        "id": "O86LFAxelcNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see some examples of prompting that are generally considered bad."
      ],
      "metadata": {
        "id": "00dpMzCqlipI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Bad Prompt Example 1 - Open-ended Prompt"
      ],
      "metadata": {
        "id": "dicmSsx8lpzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt may generate a less informative or focused response than the previous chain prompt example due to its more open-ended nature."
      ],
      "metadata": {
        "id": "VXuwvBWel0in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something interesting about {scientist}.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(template=template_fact, input_variables=[\"scientist\"])\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(f\"Scientist: {scientist}\")\n",
        "print(f\"Fact: {response_fact}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlj5luGBl8jz",
        "outputId": "ede713e4-9074-4eec-f07e-9b36939816c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scientist: Albert Einstein\n",
            "Fact: \n",
            "Albert Einstein was a passionate musician and was known to play the violin daily. He even said that if he hadn't become a scientist, he would have become a musician. He also believed that music helped him think and come up with new ideas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bad Prompt Example 2 - Unclear Prompt"
      ],
      "metadata": {
        "id": "bQ7J6VqDmfpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "# Prompt 1\n",
        "template_question = \"\"\"What are some musical genres?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something about {genre1}, {genre2}, and {genre3} without giving any specific details.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"genre1\", \"genre2\", \"genre3\"], template=template_fact)\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Assign three hardcoded genres\n",
        "genre1, genre2, genre3 = \"jazz\", \"pop\", \"rock\"\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"genre1\": genre1, \"genre2\": genre2, \"genre3\": genre3}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(f\"Genres: {genre1}, {genre2}, {genre3}\")\n",
        "print(f\"Fact: {response_fact}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esdzETJxmnTn",
        "outputId": "a5055b10-90f3-4303-8d01-2d0840e579c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genres: jazz, pop, rock\n",
            "Fact: \n",
            "Jazz, pop, and rock are three popular genres of music that have evolved over time and have a wide range of styles and influences. They all have distinct characteristics and elements that make them unique, such as improvisation in jazz, catchy melodies in pop, and heavy guitar riffs in rock. These genres have also influenced and been influenced by each other, leading to the creation of sub-genres and fusion styles. They have a strong presence in mainstream music and have been embraced by audiences of all ages and backgrounds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the second prompt is constructed poorly. It asks to \"tell me something about {genre1}, {genre2}, and {genre3} without giving any specific details.\" This prompt is unclear, as it asks for information about the genres but also states not to provide specific details. This makes it difficult for the LLM to generate a coherent and informative response. As a result, the LLM may provide a less informative or confusing answer.\n",
        "\n",
        "The first prompt asks for \"some musical genres\" **without specifying any criteria or context**, and the second prompt asks why the given genres are \"unique\" **without providing any guidance** on what aspects of uniqueness to focus on, such as their historical origins, stylistic features, or cultural significance."
      ],
      "metadata": {
        "id": "KJBbEwgZnGk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "wtoZ3iQrn1Cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Chain of Thought Prompting"
      ],
      "metadata": {
        "id": "JopNey0mn1C6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain of Thought Prompting (CoT) is a technique developed to encourage large language models to explain their reasoning process, leading to more accurate results. By providing few-shot exemplars demonstrating the reasoning process, the LLM is guided to explain its reasoning when answering the prompt. This approach has been found effective in improving results on tasks like arithmetic, common sense, and symbolic reasoning.\n",
        "\n",
        "In the context of LangChain, CoT can be beneficial for several reasons. First, it can help break down complex tasks by assisting the LLM in decomposing a complex task into simpler steps, making it easier to understand and solve the problem. This is particularly useful for calculations, logic, or multi-step reasoning tasks. Second, CoT can guide the model through related prompts, helping generate more coherent and contextually relevant outputs. This can lead to more accurate and useful responses in tasks that require a deep understanding of the problem or domain.\n",
        "\n",
        "There are some limitations to consider when using CoT. One limitation is that it has been found to yield performance gains only when used with models of approximately 100 billion parameters or larger; smaller models tend to produce illogical chains of thought, which can lead to worse accuracy than standard prompting. Another limitation is that CoT may not be equally effective for all tasks. It has been shown to be most effective for tasks involving arithmetic, common sense, and symbolic reasoning. For other types of tasks, the benefits of using CoT might be less pronounced or even counterproductive."
      ],
      "metadata": {
        "id": "Z07CuVRKoP5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "3MUNOFQJoTpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Tips for Effective Prompt Engineering"
      ],
      "metadata": {
        "id": "ytehxNw0oTpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Be specific** with your prompt: Provide enough context and detail to guide the LLM toward the desired output.\n",
        "- **Force conciseness** when needed.\n",
        "- **Encourage the model to explain its reasoning:** This can lead to more accurate results, especially for complex tasks.\n",
        "\n",
        "Keep in mind that prompt engineering is an iterative process, and it may require several refinements to obtain the best possible answer. As LLMs become more integrated into products and services, the ability to create effective prompts will be an important skill to have.\n",
        "\n",
        "A well-structured prompt example is given below:"
      ],
      "metadata": {
        "id": "kO0PhKz8oeuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the secret to happiness?\",\n",
        "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
        "    }, {\n",
        "        \"query\": \"How can I become more productive?\",\n",
        "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the few-shot prompt template\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
        "\n",
        "# Define the user query\n",
        "user_query = \"What are some tips for improving communication skills?\"\n",
        "\n",
        "# Run the LLMChain for the user query\n",
        "response = chain.run({\"query\": user_query})\n",
        "\n",
        "print(f\"User Query: {user_query}\")\n",
        "print(f\"AI Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r3PWBurowVy",
        "outputId": "4671618e-c86e-4483-c33a-e79d0971465a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: What are some tips for improving communication skills?\n",
            "AI Response: Active listening, being clear and concise, and practicing empathy are all important skills for effective communication. Also, don't be afraid to ask for clarification or feedback to improve your communication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt:\n",
        "\n",
        "- **Provides a clear context in the prefix:** The prompt states that the AI is a life coach providing insightful and practical advice. This context helps guide the AI's responses and ensures they align with the intended purpose.\n",
        "\n",
        "- **Uses examples that demonstrate the AI's role and the type of responses it generates:** By providing relevant examples, the AI can better understand the style and tone of the responses it should produce. These examples serve as a reference for the AI to generate similar responses that are consistent with the given context.\n",
        "\n",
        "- **Separates examples and the actual query:** This allows the AI to understand the format it should follow, ensuring a clear distinction between example conversations and the user's input. This separation helps the AI to focus on the current query and respond accordingly.\n",
        "\n",
        "- **Includes a clear suffix that indicates where the user's input goes and where the AI should provide its response:** The suffix acts as a cue for the AI, showing where the user's query ends and the AI's response should begin. This structure helps maintain a clear and consistent format for the generated responses.\n",
        "\n",
        "By using this well-structured prompt, the AI can understand its role, the context, and the expected response format, leading to more accurate and useful outputs."
      ],
      "metadata": {
        "id": "QiRombcEpvkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "gOaogb50qbYV"
      }
    }
  ]
}