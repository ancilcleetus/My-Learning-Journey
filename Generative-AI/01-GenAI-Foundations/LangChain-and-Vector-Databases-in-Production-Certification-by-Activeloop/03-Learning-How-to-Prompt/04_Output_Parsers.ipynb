{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📖 TABLE OF CONTENTS\n",
        "\n",
        "- [1. Introduction]()\n",
        "- [2. Installing Dependencies]()\n",
        "- [3. Mount Google Drive & Load API Keys]()\n",
        "- [4. Output Parsers]()\n",
        "  - [1. `PydanticOutputParser`]()\n",
        "    - [Multiple Outputs Example]()\n",
        "  - [2. `CommaSeparatedListOutputParser`]()\n",
        "  - [3. `StructuredOutputParser`]()\n",
        "- [5. Fixing Errors]()\n",
        "  - [1. OutputFixingParser]()\n",
        "  - [2. RetryOutputParser]()\n",
        "- [6. Conclusion]()"
      ],
      "metadata": {
        "id": "nE1NTpTjEitC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the language models can only generate textual outputs, a predictable data structure is always preferred in a production environment. For example, imagine you are creating a thesaurus application and want to generate a list of possible substitute words based on the context. The LLMs are powerful enough to generate many suggestions easily. Here is a sample output from the ChatGPT for several words with close meaning to the term “behavior.”"
      ],
      "metadata": {
        "id": "haaG8kwh6UQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Here are some substitute words for \"behavior\":\n",
        "\n",
        "Conduct\n",
        "Manner\n",
        "Demeanor\n",
        "Attitude\n",
        "Disposition\n",
        "Deportment\n",
        "Etiquette\n",
        "Protocol\n",
        "Performance\n",
        "Actions"
      ],
      "metadata": {
        "id": "CExWMHwW6Xi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is the lack of a method to extract relevant information from the mentioned string dynamically. You might say we can split the response by a new line and ignore the first two lines. However, there is no guarantee that the response have the same format every time. The list might be numbered, or there could be no introduction line.\n",
        "\n",
        "The Output Parsers help create a data structure to define the expectations from the output precisely. We can ask for a list of words in case of the word suggestion application or a combination of different variables like a word and the explanation of why it fits. The parser can extract the expected information for you.\n",
        "\n",
        "This lesson covers the different types of parsing objects and the troubleshooting processing."
      ],
      "metadata": {
        "id": "5-zo8MuS6dX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "Fv4x-kn2bvXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Installing Dependencies"
      ],
      "metadata": {
        "id": "iSMXICypS_jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain==0.0.208 deeplake openai==0.27.8 python-dotenv tiktoken"
      ],
      "metadata": {
        "id": "c6qXOrjxSqa_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c5bd679-bdba-4657-f44d-12b12be76b87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting deeplake\n",
            "  Downloading deeplake-3.9.15.tar.gz (607 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.10.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pydantic<2,>=1 (from langchain==0.0.208)\n",
            "  Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.4)\n",
            "Collecting pillow~=10.2.0 (from deeplake)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.149-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.10/dist-packages (from deeplake) (2.8.0)\n",
            "Collecting libdeeplake==0.0.137 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.137-cp310-cp310-manylinux2014_x86_64.whl.metadata (352 bytes)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-13.1.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\n",
            "Collecting dill (from libdeeplake==0.0.137->deeplake)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting aiobotocore==2.13.1 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.13.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting botocore<1.34.132,>=1.34.70 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.34.131-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.131-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (3.0.3)\n",
            "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pox>=0.3.4 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libdeeplake-0.0.137-cp310-cp310-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioboto3-13.1.1-py3-none-any.whl (34 kB)\n",
            "Downloading aiobotocore-2.13.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.34.131-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.9.15-py3-none-any.whl size=730462 sha256=fdc438ccc00b287e66def1eab2d93d9639aac9b760c3ff2dd99420a6a4c3f7bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/99/d8/463107fea3454d11c8c9cc8e621ff3a29a4790aeeb27496149\n",
            "Successfully built deeplake\n",
            "Installing collected packages: python-dotenv, pydantic, ppft, pox, pillow, mypy-extensions, marshmallow, lz4, jmespath, dill, aioitertools, aiofiles, typing-inspect, tiktoken, openapi-schema-pydantic, multiprocess, libdeeplake, langchainplus-sdk, humbug, botocore, s3transfer, pathos, openai, dataclasses-json, aiobotocore, langchain, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.11 requires pydantic>=2.7.0, but you have pydantic 1.10.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-13.1.1 aiobotocore-2.13.1 aiofiles-24.1.0 aioitertools-0.11.0 boto3-1.34.131 botocore-1.34.131 dataclasses-json-0.5.14 deeplake-3.9.15 dill-0.3.8 humbug-0.3.2 jmespath-1.0.1 langchain-0.0.208 langchainplus-sdk-0.0.20 libdeeplake-0.0.137 lz4-4.3.3 marshmallow-3.21.3 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pathos-0.3.2 pillow-10.2.0 pox-0.3.4 ppft-1.7.6.8 pydantic-1.10.17 python-dotenv-1.0.1 s3transfer-0.10.2 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "50380dfdf85b40f6ac295c9b489219f4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "v6uUrMvsbxfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Mount Google Drive & Load API Keys"
      ],
      "metadata": {
        "id": "offx4yVJZCZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YY4yDKPpZGQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4891e9-9947-4f44-da39-98e091877361"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the API Keys are stored in file \"llm_env\". It's contents are as below:\n",
        "\n",
        "ACTIVELOOP_TOKEN=<[Your Activeloop API Key](https://app.activeloop.ai/register)>\n",
        "\n",
        "OPENAI_API_KEY=<[Your OpenAI API Key](https://platform.openai.com/)>\n",
        "\n",
        "GOOGLE_API_KEY=<[Your Google API Key](https://console.cloud.google.com/apis/credentials)>\n",
        "\n",
        "GOOGLE_CSE_ID=<[Your Google Custom Search Engine ID](https://programmablesearchengine.google.com/controlpanel/create)>\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN=<[Your Hugging Face Access Token](https://huggingface.co/settings/tokens)>"
      ],
      "metadata": {
        "id": "YWHIdr_BeSwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Keys for Deep Lake Vector Database, OpenAI, Google & Hugging Face\n",
        "load_dotenv('/content/drive/MyDrive/ancilcleetus-github/llm_env')"
      ],
      "metadata": {
        "id": "dWCwbr3UbsD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930fc551-60d2-4c9d-c576-2aeeaa176d6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(f\"os.environ['ACTIVELOOP_TOKEN']: \\n{os.environ['ACTIVELOOP_TOKEN']}\")\n",
        "print(f\"os.environ['OPENAI_API_KEY']: \\n{os.environ['OPENAI_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_API_KEY']: \\n{os.environ['GOOGLE_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_CSE_ID']: \\n{os.environ['GOOGLE_CSE_ID']}\")\n",
        "print(f\"os.environ['HUGGINGFACEHUB_API_TOKEN']: \\n{os.environ['HUGGINGFACEHUB_API_TOKEN']}\")"
      ],
      "metadata": {
        "id": "e2XY9JzccqGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "z7H7VG4v7mhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Output Parsers"
      ],
      "metadata": {
        "id": "Q7ybCVt97mhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three classes that we will introduce in this section. While the Pydrantic parser is the most powerful and flexible wrapper, knowing the other options for less complicated problems is beneficial. We will implement the thesaurus application in each section to better understand the details of each approach."
      ],
      "metadata": {
        "id": "87MoLBWf7tIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. `PydanticOutputParser`"
      ],
      "metadata": {
        "id": "IngZuN1R72yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class instructs the model to generate its output in a JSON format and then extract the information from the response. You will be able to treat the parser's output as a list, meaning it will be possible to index through the results without worrying about formatting."
      ],
      "metadata": {
        "id": "rKjad0yh8DJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class uses the Pydantic library, which helps define and validate data structures in Python. It enables us to characterize the expected output with a name, type, and description. We need a variable that can store multiple suggestions in the thesaurus example. It can be easily done by defining a class that inherits from the Pydantic’s BaseModel class."
      ],
      "metadata": {
        "id": "jWNCowKM8SBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "\n",
        "    # Throw error in case of receiving a numbered-list from API\n",
        "    @validator('words')\n",
        "    def not_start_with_number(cls, field):\n",
        "        for item in field:\n",
        "            if item[0].isnumeric():\n",
        "                raise ValueError(\"The word can not start with numbers!\")\n",
        "        return field\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Suggestions)"
      ],
      "metadata": {
        "id": "d9MqwYuB85tx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We always import and follow the necessary libraries by creating the `Suggestions` schema class. There are two essential parts to this class:\n",
        "\n",
        "1. **Expected Outputs:** Each output is defined by declaring a variable with desired type, like a list of strings (`: List[str]`) in the sample code, or it could be a single string (`: str`) if you are expecting just one word/sentence as the response. Also, It is required to write a simple explanation using the `Field` function's `description` attribute to help the model during inference. (We will see an example of having multiple outputs later in the lesson)\n",
        "\n",
        "2. **Validators:** It is possible to declare functions to validate the formatting. We ensure that the first character is not a number in the sample code. The function's name is unimportant, but the `@validator` decorator must receive the same name as the variable you want to approve. (like `@validator('words')`) It is worth noting that the `field` variable inside the validator function will be a list if you specify it as one."
      ],
      "metadata": {
        "id": "aBFzZP1n9CH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will pass the created class to the `PydanticOutputParser` wrapper to make it a LangChain parser object. The next step is to prepare the prompt."
      ],
      "metadata": {
        "id": "6Wrg2jy-9H_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitue the specified target_word based the presented context.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "model_input = prompt.format_prompt(\n",
        "\t\t\ttarget_word=\"behaviour\",\n",
        "\t\t\tcontext=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
        ")"
      ],
      "metadata": {
        "id": "JyUjCkqX9yLb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in previous lessons, the `template` variable is a string that can have named index placeholders using the following `{variable_name}` format. The template outlines our expectations for the model, including the expected formatting from the parser and the inputs. The `PromptTemplate` receives the template string with the details of each placeholder's type. They could either be 1) `input_variables` whose value is initialized later on using the `.format_prompt()` function, or 2) `partial_variables` to be initialized instantly."
      ],
      "metadata": {
        "id": "s8YmkPBG-Eaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "model = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.0)\n",
        "\n",
        "output = model(model_input.to_string())\n",
        "\n",
        "parser.parse(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnwI8nsK-UK1",
        "outputId": "871fa409-8b5d-4f2c-f2fc-d97be9673db5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manage', 'handle', 'oversee', 'supervise'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parser object's `parse()` function will convert the model's string response to the format we specified. There is a list of words that you can index through and use in your applications."
      ],
      "metadata": {
        "id": "aMoSRN9t-o43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Outputs Example"
      ],
      "metadata": {
        "id": "3wFLXMtr-32b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a sample code for Pydantic class to process multiple outputs. It requests the model to suggest a list of words and present the reasoning behind each proposition.\n",
        "\n",
        "Replace the `template` variable and `Suggestion` class with the following codes to run this example. The template changes will ask the model to present its reasoning, and the suggestion class declares a new output named `reasons`. Also, the validator function manipulates the output to ensure every reasoning ends with a dot. Another use case of the validator function could be output manipulation."
      ],
      "metadata": {
        "id": "JyxjX0FY_KXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitute the specified target_word based on the presented context and the reasoning for each word.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ajlMZlZo_R1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "    @validator('words')\n",
        "    def not_start_with_number(cls, field):\n",
        "      for item in field:\n",
        "        if item[0].isnumeric():\n",
        "          raise ValueError(\"The word can not start with numbers!\")\n",
        "      return field\n",
        "\n",
        "    @validator('reasons')\n",
        "    def end_with_dot(cls, field):\n",
        "      for idx, item in enumerate( field ):\n",
        "        if item[-1] != \".\":\n",
        "          field[idx] += \".\"\n",
        "      return field"
      ],
      "metadata": {
        "id": "avAOfibu_UFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full code is as below:"
      ],
      "metadata": {
        "id": "rJhEtc_6_iwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "    @validator('words')\n",
        "    def not_start_with_number(cls, field):\n",
        "      for item in field:\n",
        "        if item[0].isnumeric():\n",
        "          raise ValueError(\"The word can not start with numbers!\")\n",
        "      return field\n",
        "\n",
        "    @validator('reasons')\n",
        "    def end_with_dot(cls, field):\n",
        "      for idx, item in enumerate( field ):\n",
        "        if item[-1] != \".\":\n",
        "          field[idx] += \".\"\n",
        "      return field\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Suggestions)"
      ],
      "metadata": {
        "id": "DJRnErvX_lRY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitute the specified target_word based on the presented context and the reasoning for each word.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "model_input = prompt.format_prompt(\n",
        "\t\t\ttarget_word=\"behaviour\",\n",
        "\t\t\tcontext=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
        ")"
      ],
      "metadata": {
        "id": "GrbIJYob_svp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "model = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.0)\n",
        "\n",
        "output = model(model_input.to_string())\n",
        "\n",
        "parser.parse(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsJmcPs0_1fB",
        "outputId": "e75d14ad-fd8f-4de9-8fb8-bc37183c42e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manage', 'handle', 'oversee'], reasons=[\"These words all imply a sense of control and authority, which is lacking in the original context. They also suggest a more active role in guiding the students' actions.\", 'These words all suggest a more organized and structured approach to the situation, which contrasts with the disruptive behaviour of the students.', 'These words all imply a sense of responsibility and leadership, which is lacking in the original context. They also suggest a more proactive approach to addressing the issue.', \"These words all suggest a more authoritative and assertive approach to managing the students' behaviour, which may be necessary in this situation.\"])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. `CommaSeparatedListOutputParser`"
      ],
      "metadata": {
        "id": "zg_7PTHc__7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident from the name of this class that it manages comma-separated outputs. It handles one specific case: anytime you want to receive a list of outputs from the model."
      ],
      "metadata": {
        "id": "ORfhHzCHAImp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "parser = CommaSeparatedListOutputParser()"
      ],
      "metadata": {
        "id": "ktLfrh63AK4k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Prepare the Prompt\n",
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitute the word '{target_word}' based the presented the following text: {context}.\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "model_input = prompt.format(\n",
        "  target_word=\"behaviour\",\n",
        "  context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
        ")\n",
        "\n",
        "# Loading OpenAI API\n",
        "model = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.0)\n",
        "\n",
        "# Send the Request\n",
        "output = model(model_input)\n",
        "parser.parse(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpnZtRdgAR8x",
        "outputId": "d07d0024-0643-4a3a-a046-cff70f6fa026"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Conduct\\n2. Manner\\n3. Demeanor\\n4. Conducting\\n5. Attitude\\n6. Conductance\\n7. Deportment\\n8. Etiquette\\n9. Performance\\n10. Actions']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although most of the sample code has been explained in the previous subsection, two parts might need attention. Firstly, we tried a new format for the prompt's template to show different ways to write a prompt. Secondly, the use of `.format()` instead of `.format_prompt()` to generate the model's input. The main difference compared to the previous subsection's code is that we no longer need to call the `.to_string()` object since the prompt is already in string type.\n",
        "\n",
        "As you can see, the final output is a list of words that has some overlaps with the `PydanticOutputParser` approach with more variety. However, requesting additional reasoning information using the `CommaSeparatedListOutputParser` class is impossible."
      ],
      "metadata": {
        "id": "LYK6EPltAmlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. `StructuredOutputParser`"
      ],
      "metadata": {
        "id": "m9chivKWA7eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first output parser implemented by the LangChain team. While it can process multiple outputs, it only supports texts and does not provide options for other data types, such as lists or integers. It can be used when you want to receive one response from the model. For example, only one substitute word in the thesaurus application."
      ],
      "metadata": {
        "id": "VlbMYOyfBDwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"words\", description=\"A substitue word based on context\"),\n",
        "    ResponseSchema(name=\"reasons\", description=\"the reasoning of why this word fits the context.\")\n",
        "]\n",
        "\n",
        "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "UhIIPZ2XBGd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class has no advantage since the `PydanticOutputParser` class provides validation and more flexibility for more complex tasks, and the `CommaSeparatedListOutputParser` option covers more straightforward applications."
      ],
      "metadata": {
        "id": "LqH492m4BKqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "xhyFJonMBkA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Fixing Errors"
      ],
      "metadata": {
        "id": "6W-Op1_vBkBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parsers are powerful tools to dynamically extract the information from the prompt and validate it to some extent. Still, they do not guarantee a response. Imagine a situation where you deployed your application, and the model's response [to a user’s request] is incomplete, causing the parser to throw an error. It is not ideal! In the following subsections, we will introduce two classes acting as fail-safe. They add a layer on top of the model's response to help fix the errors.\n",
        "\n",
        "**Note**\n",
        "\n",
        "The following approaches work with the `PydanticOutputParser` class since it is the only one with a validation method."
      ],
      "metadata": {
        "id": "Pm2EggdyBv2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. OutputFixingParser"
      ],
      "metadata": {
        "id": "2OCtrGXPB9Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method tries to fix the parsing error by looking at the model’s response and the previous parser. It uses a Large Language Model (LLM) to solve the issue. We will use GPT-3 to be consistent with the rest of the lesson, but it is possible to pass any supported model. Let's start by defining the Pydantic data schema and show a sample error that could occur."
      ],
      "metadata": {
        "id": "WkvL7xv3CDZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
        "\n",
        "missformatted_output = '{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'\n",
        "\n",
        "parser.parse(missformatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "rLOVBjQeCHU_",
        "outputId": "6e487ea9-32cf-4cb2-d3e3-87e7941e13df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-dc9240a19f4b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmissformatted_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissformatted_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the error message, the parser correctly identified an error in our sample response (`missformatted_output`) since we used the word `reasoning` instead of the expected `reasons` key. The `OutputFixingParser` class could easily fix this error."
      ],
      "metadata": {
        "id": "Kj6VgwX4COj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import OutputFixingParser\n",
        "\n",
        "model = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.0)\n",
        "\n",
        "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
        "outputfixing_parser.parse(missformatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyNzw-PvCX_j",
        "outputId": "b602aa0f-40ae-4dfc-b37d-1b87d207442d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manner'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `from_llm()` function takes the old parser and a language model as input parameters. Then, It initializes a new parser for you that has the ability to fix output errors. In this case, it successfully identified the misnamed key and changed it to what we defined."
      ],
      "metadata": {
        "id": "hvtvvtuXCiFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, fixing the issues using this class is not always possible. Here is an example of using `OutputFixingParser` class to resolve an error with a missing key."
      ],
      "metadata": {
        "id": "p_eEBOxrCndD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'\n",
        "\n",
        "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
        "\n",
        "outputfixing_parser.parse(missformatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIQxr182CrRJ",
        "outputId": "00aad108-b177-4741-afbc-c0c5be4fc9b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manner'], reasons=['These words both describe a way of behaving or carrying oneself.'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the output, it is evident that the model understood the key `reasons` missing from the response but didn't have the context of the desired outcome. It created a list with one entry, while we expect one reason per word. This is why we sometimes need to use the `RetryOutputParser` class."
      ],
      "metadata": {
        "id": "ruGwO-NlCxPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. RetryOutputParser"
      ],
      "metadata": {
        "id": "q-zJoqA3C7SD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some cases, the parser needs access to both the output and the prompt to process the full context, as demonstrated in the previous section."
      ],
      "metadata": {
        "id": "VzMh8vsBDBkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Define data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
        "\n",
        "# Define prompt\n",
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "model_input = prompt.format_prompt(target_word=\"behaviour\", context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")\n",
        "\n",
        "# Define Model\n",
        "model = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.0)"
      ],
      "metadata": {
        "id": "EaNIM5UcDGIM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can fix the same `missformatted_output` using the `RetryWithErrorOutputParser` class. It receives the old parser and a model to declare the new parser object, as we saw in the previous section. However, the `parse_with_prompt` function is responsible for fixing the parsing issue while requiring the output and the prompt."
      ],
      "metadata": {
        "id": "pjXlHknaDNDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RetryWithErrorOutputParser\n",
        "\n",
        "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'\n",
        "\n",
        "retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=model)\n",
        "\n",
        "retry_parser.parse_with_prompt(missformatted_output, model_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdk1uSbLDUyg",
        "outputId": "ea3abc14-bfa0-4190-eab0-dda118d3b1ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manner'], reasons=['These words both convey a sense of control and order, which is the opposite of disruptive behaviour in a classroom setting.', 'Both words also imply a level of professionalism and respect, which is important in a classroom environment.'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outputs show that the `RetryOutputParser` has the ability to fix the issue where the `OuputFixingParser` was not able to. The parser correctly guided the model to generate one reason for each word.\n",
        "\n",
        "The best practice to incorporate these techniques in production is to catch the parsing error using a `try: ... except: ...` method. It means we can capture the errors in the `except` section and attempt to fix them using the mentioned classes. It will limit the number of API calls and avoid unnecessary costs that are associated with it."
      ],
      "metadata": {
        "id": "7tkxChd1DdHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "guua8NXuD9z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conclusion"
      ],
      "metadata": {
        "id": "dnNDvR-WD90K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learned how to validate and extract the information in an easy-to-use format from the language models' responses which are always a string. Additionally, we reviewed LangChain's fail-safe procedures to guarantee the consistency of the output. Combining these approaches will help us write more reliable applications in production environments."
      ],
      "metadata": {
        "id": "6_1EDFbvEHL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "U897-NGnYhHk"
      }
    }
  ]
}