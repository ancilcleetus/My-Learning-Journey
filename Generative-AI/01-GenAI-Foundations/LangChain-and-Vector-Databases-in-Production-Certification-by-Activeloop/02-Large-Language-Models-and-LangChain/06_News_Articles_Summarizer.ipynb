{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's fast-paced world, it's essential to stay updated with the latest news and information. However, going through multiple news articles can be time-consuming. To help you save time and get a quick overview of the important points, letâ€™s develop a News Articles Summarizer application using ChatGPT and LangChain. With this powerful tool, we can scrape online articles, extract their titles and text, and generate concise summaries. Within this lesson, we will walk you through the workflow of constructing a summarizer. We will employ the concepts we discussed in earlier lessons, demonstrating their application in a real-world scenario."
      ],
      "metadata": {
        "id": "iZ3hvDXWibBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "Fv4x-kn2bvXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Installing Dependencies"
      ],
      "metadata": {
        "id": "iSMXICypS_jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain==0.0.208 openai==0.27.8 python-dotenv newspaper3k"
      ],
      "metadata": {
        "id": "c6qXOrjxSqa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "v6uUrMvsbxfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Mount Google Drive & Load API Keys"
      ],
      "metadata": {
        "id": "offx4yVJZCZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YY4yDKPpZGQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the API Keys are stored in file \"llm_env\". It's contents are as below:\n",
        "\n",
        "ACTIVELOOP_TOKEN=<[Your Activeloop API Key](https://app.activeloop.ai/register)>\n",
        "\n",
        "OPENAI_API_KEY=<[Your OpenAI API Key](https://platform.openai.com/)>\n",
        "\n",
        "GOOGLE_API_KEY=<[Your Google API Key](https://console.cloud.google.com/apis/credentials)>\n",
        "\n",
        "GOOGLE_CSE_ID=<[Your Google Custom Search Engine ID](https://programmablesearchengine.google.com/controlpanel/create)>\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN=<[Your Hugging Face Access Token](https://huggingface.co/settings/tokens)>"
      ],
      "metadata": {
        "id": "YWHIdr_BeSwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Keys for Deep Lake Vector Database, OpenAI, Google & Hugging Face\n",
        "load_dotenv('/content/drive/MyDrive/ancilcleetus-github/llm_env')"
      ],
      "metadata": {
        "id": "dWCwbr3UbsD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(f\"os.environ['ACTIVELOOP_TOKEN']: \\n{os.environ['ACTIVELOOP_TOKEN']}\")\n",
        "print(f\"os.environ['OPENAI_API_KEY']: \\n{os.environ['OPENAI_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_API_KEY']: \\n{os.environ['GOOGLE_API_KEY']}\")\n",
        "print(f\"os.environ['GOOGLE_CSE_ID']: \\n{os.environ['GOOGLE_CSE_ID']}\")\n",
        "print(f\"os.environ['HUGGINGFACEHUB_API_TOKEN']: \\n{os.environ['HUGGINGFACEHUB_API_TOKEN']}\")"
      ],
      "metadata": {
        "id": "e2XY9JzccqGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "spQ5b1pBi2pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Workflow for Building a News Articles Summarizer"
      ],
      "metadata": {
        "id": "EnGOEk-Ti2pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "C3kG_6ZmimM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning as subset of ML\n",
        "\n",
        "from IPython import display\n",
        "display.Image(\"data/images/DL_01_Intro-01-DL-subset-of-ML.jpg\")"
      ],
      "metadata": {
        "id": "juNAxVBFg7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}