{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Transfer Learning and Fine-tuning: Background"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Problems with training our own model\n",
        "    - Deep Learning is very costly since it requires\n",
        "        - lots of data\n",
        "        - lots of time for training\n",
        "        - lots of GPU compute\n",
        "    - No specific way to get best performing model for our dataset\n",
        "\n",
        "        Ambiguity with regards to\n",
        "        - Which kernel size to take ?\n",
        "        - How many kernels per layer ?\n",
        "        - How many layers ?\n",
        "        - Which activation to use ?\n",
        "\n",
        "- Solution:\n",
        "    \n",
        "    - Scientists did a lot of experiments with lots of data and with lots of hyperparameter choices (kernel size, no of kernels, no of layers, activation etc.) and came up with Deep Learning models that have very high accuracy on gigantic datasets.\n",
        "\n",
        "    - ILSVRC (ImageNet Large Scale Visual Recognition Challenge):\n",
        "        - ImageNet dataset has 1.4 Million images in 1000 classes\n",
        "        - Build a Deep Learning model that has the most accuracy on ImageNet dataset\n",
        "\n",
        "    - If this Deep Learning model performs really well in a gigantic dataset like ImageNet with more no of images and more no of classes, why can't we use this for our dataset (of course with less no of images and less no of classes) ?"
      ],
      "metadata": {
        "id": "hVH5PreNCXzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Transfer Learning\n",
        "\n",
        "- Technique that focuses on storing knowledge gained while solving one problem and applying it to a different problem\n",
        "- With Transfer Learning, we can train a Deep Learning model faster in comparison to training the model from scratch. This is because the pretrained model has already learned the low level features really well. These low level features are common for any Computer Vision task. Hence, we just need to fine tune the high level features of the pretrained model for our dataset while keeping the low level features intact."
      ],
      "metadata": {
        "id": "yVswruiSWDCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transfer Learning in Action"
      ],
      "metadata": {
        "id": "_Z4nnBxOj24O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do Transfer Learning in two ways:\n",
        "\n",
        "- When our dataset is similar to ImageNet dataset:\n",
        "    1. Freeze all Convolution layers\n",
        "    2. Modify and fine-tune Fully Connected layers for our dataset\n",
        "        - For example, if our dataset has 10 classes, we need to use a Fully Connected layer of 10 perceptrons instead of 1000 perceptrons.\n",
        "- When our dataset is not similar to ImageNet dataset:\n",
        "    1. Freeze initial Convolution layers that extract low level features which are common for any Computer Vision problem (such as edges, basic shapes etc.) and fine-tune remaining Convolution layers\n",
        "        - For example, freeze first 3 Convolution layers & fine-tune 4th and 5th Convolution layers.\n",
        "    2. Modify and fine-tune Fully Connected layers for our dataset\n",
        "\n",
        "Since we are freezing many Convolution layers, no of parameters to be trained is significantly reduced. Hence, fine-tuning can be done faster in comparison to training from scratch."
      ],
      "metadata": {
        "id": "g-E2YQiiem39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning as subset of ML\n",
        "\n",
        "from IPython import display\n",
        "display.Image(\"data/images/DL_01_Intro-01-DL-subset-of-ML.jpg\")"
      ],
      "metadata": {
        "id": "juNAxVBFg7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}