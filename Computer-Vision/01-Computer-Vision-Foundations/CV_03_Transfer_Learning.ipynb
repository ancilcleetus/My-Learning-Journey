{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– TABLE OF CONTENTS\n",
        "\n",
        "- [1. Transfer Learning and Fine-tuning: Background]()\n",
        "- [2. Transfer Learning]()\n",
        "- [3. Transfer Learning in Action]()"
      ],
      "metadata": {
        "id": "2Kph3Pq0KBgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "uLR90WhedwER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Transfer Learning and Fine-tuning: Background"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Problems with training our own model\n",
        "    - Deep Learning is very costly since it requires\n",
        "        - lots of data\n",
        "        - lots of time for training\n",
        "        - lots of GPU compute\n",
        "    - No specific way to get best performing model for our dataset\n",
        "\n",
        "        Ambiguity with regards to\n",
        "        - Which kernel size to take ?\n",
        "        - How many kernels per layer ?\n",
        "        - How many layers ?\n",
        "        - Which activation to use ?\n",
        "\n",
        "- Solution:\n",
        "    \n",
        "    - Scientists did a lot of experiments with lots of data and with lots of hyperparameter choices (kernel size, no of kernels, no of layers, activation etc.) and came up with Deep Learning models that have very high accuracy on gigantic datasets.\n",
        "\n",
        "    - ILSVRC (ImageNet Large Scale Visual Recognition Challenge):\n",
        "        - ImageNet dataset has 1.4 Million images in 1000 classes\n",
        "        - Build a Deep Learning model that has the most accuracy on ImageNet dataset\n",
        "\n",
        "    - If this Deep Learning model performs really well in a gigantic dataset like ImageNet with more no of images and more no of classes, why can't we use this for our dataset (of course with less no of images and less no of classes) ?"
      ],
      "metadata": {
        "id": "hVH5PreNCXzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Transfer Learning\n",
        "\n",
        "- Technique that focuses on storing knowledge gained while solving one problem and applying it to a different problem.\n",
        "- The pretrained models which were trained on gigantic datasets like ImageNet have already learned the low level features really well. These low level features are more or less common for any Computer Vision task. Hence, we freeze the lower layers in order to keep the low level features intact. We then fine-tune the higher layers to extract higher level features that are unique to our dataset.\n",
        "- Since no of parameters that we need to train has now been reduced by a significant amount, training will be faster in comparison to training from scratch."
      ],
      "metadata": {
        "id": "yVswruiSWDCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transfer Learning in Action"
      ],
      "metadata": {
        "id": "_Z4nnBxOj24O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do Transfer Learning in two ways:\n",
        "\n",
        "- When our dataset is similar to ImageNet dataset:\n",
        "    1. Freeze all Convolution layers\n",
        "    2. Modify and fine-tune Fully Connected layers for our dataset\n",
        "        - For example, lets say we use VGG16 pretrained model for Transfer Learning. If our dataset has 10 classes, we need to use a Fully Connected layer of 10 perceptrons instead of 1000 perceptrons just before the final Softmax layer.\n",
        "- When our dataset is not similar to ImageNet dataset:\n",
        "    1. Freeze initial Convolution layers that extract low level features which are common for any Computer Vision problem (such as edges, basic shapes etc.) and fine-tune remaining Convolution layers\n",
        "        - For VGG16 pretrained model, we can try freezing the first 3 Convolution layers while fine-tuning the 4th and 5th Convolution layers.\n",
        "    2. Modify and fine-tune Fully Connected layers for our dataset\n",
        "\n",
        "Since we are freezing many Convolution layers, no of parameters to be trained is significantly reduced. Hence, fine-tuning can be done faster in comparison to training from scratch."
      ],
      "metadata": {
        "id": "g-E2YQiiem39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rainbow](https://github.com/ancilcleetus/My-Learning-Journey/assets/25684256/839c3524-2a1d-4779-85a0-83c562e1e5e5)"
      ],
      "metadata": {
        "id": "qqv0L0R9dyKJ"
      }
    }
  ]
}