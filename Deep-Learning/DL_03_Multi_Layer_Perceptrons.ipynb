{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. No of Trainable Parameters in a Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "bKim3KwyTp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of trainable parameters in a Multilayer Perceptron (MLP) can be calculated by considering the connections between neurons in each layer, including the weights and biases.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "1. **Number of Weights**: For each connection between neurons in adjacent layers, there is a weight. If you have $n_i$ neurons in the $i^{th}$ layer and $n_{i+1}$ neurons in the following $(i+1)^{th}$ layer, there are $n_i \\times n_{i+1}$ weights. This applies to all layers except the output layer.\n",
        "2. **Number of Biases**: Each neuron (except those in the input layer) has a bias term associated with it. So, for $i^{th}$ layer with $n_i$ neurons, there are $n_i$ biases.\n",
        "3. **Total Number of Trainable Parameters**: Summing up the weights and biases across all layers, we get the total number of trainable parameters.\n",
        "\n",
        "The equation for the number of trainable parameters in an MLP with $L$ layers (including the input and output layers) is given below:\n",
        "\n",
        "Total Parameters = $\\sum_{i=1}^{L-1} n_i \\times n_{i+1} + \\sum_{i=2}^{L} n_i$ where $n_i$ represents the number of neurons in the $i^{th}$ layer."
      ],
      "metadata": {
        "id": "SbkT5xFXbRBc"
      }
    }
  ]
}